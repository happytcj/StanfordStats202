---
title: "HW1"
output:
 pdf_document:
  latex_engine: xelatex
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
plot(cars)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file).

```{r}
tmp <- 1+1
```

Is this just blank text? If not in a chunk... Interesting


Problem 4

Exercise 1 (p. 413)

Problem 5

Exercise 2 (p. 413)

Problem 6

Exercise 4 (p. 414)

Problem 7

Exercise 9 (p. 416)

Problem 9

Exercise 9 (p. 122). In parts (e) and (f), you need only try a few interactions and transformations.

Problem 10

Exercise 14 (p. 125)

Problem 1: 
Explain whether each scenario is a classification or regression prob- lem, and indicate whether we are most interested in inference or pre- diction. Finally, provide n and p.


(a) We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.

Regression, inference, n = 500, p = 4

(b) We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each prod- uct we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.

Classification
Prediction
n = 20, p = 14, 

(c) We are interesting in predicting the % change in the US dollar in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the dollar, the % change in the US market, the % change in the British market, and the % hange in the German market.

Problem 2

Complete Exercise 3 from section 2.4 of the textbook (p. 52).

3. We now revisit the bias-variance decomposition.
(a) Provide a sketch of typical (squared) bias, variance, training er- ror, test error, and Bayes (or irreducible) error curves, on a sin- gle plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should representthe amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one.
(b) Explain why each of the five curves has the shape displayed in part (a).
i. bias is inversely related to flexibility as higher flexibility creates a closer fit

ii. variance - increases monotonically because increases in flexibility yield overfitting

iii. training error - decreases monotonically because increases in flexibility yield a closer fit

iv. test error - concave up curve because increase in flexibility yields a closer fit before it overfits

v. Bayes (irreducible) error - defines the lower limit, the test error is bounded 
below by the irreducible error due to variance in the error (epsilon) in the output 
values (0 <= value). When the training error is lower than the irreducible error,
overfitting has taken place.
The Bayes error rate is defined for classification problems and is determined by 
the ratio of data points which lie at the 'wrong' side of the decision boundary, 
(0 <= value < 1).

**Problem 3**

Complete Exercise 7 from section 2.4 of the textbook (p. 53).

```{r}
Obs <- seq(1,6,length=6)
X1 <- c(0,2,0,0,-1,1)
X2 <- c(3,0,1,1,0,1)
X3 <- c(0,0,3,2,1,1)
Y <- c('Red', 'Red', 'Red', 'Green', 'Green', 'Red')
df <- data.frame(Obs, X1, X2, X3, Y)
(df)
```

```{r}
coords_mat <- data.matrix(df[2:4])
coords_mat
```

```{r}
# df$dist <- dist(rbind(x1, x2))
zero_mat <- matrix(NA, nrow = 6, ncol = 3)
# tmp <- dist(rbind(zero_mat, coords_mat))
dist <- apply(coords_mat, 2, dist, zero_mat)
```

```{r}
library(fields)
library(SpatialTools)
# dist(cbind(c(0,0,0)), cbind(c(1,1,1)))
dist(rbind(c(0,1,3), c(0,0,0)))
```

```{r}
dist <- NULL
for(i in 1:nrow(coords_mat)) {
    dist[i] <- dist(rbind(coords_mat[i,], c(0,0,0)))
}
dist
```


Problem 8

Exercise 4 (p. 120)

4. I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. Y = β0 +β1X +β2X2 +β3X3 +ε.
(a) Suppose that the true relationship between X and Y is linear, i.e. Y = β0 + β1X + ε. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.
Adding more variables to the least squares equations always improves the fit to the training data; thus, the RSS to training data should decrease
(b) Answer (a) using test rather than training RSS.
test RSS should decrease due to the overfitting and failing to generalize overfit model to test dataset 
(c) Suppose that the truerelationshipbetweenXandYisnotlinear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.
The increased flexibility from polynomial regression will lead to a better fit to training data over a linear regression. 
(d) Answer (c) using test rather than training RSS.
Since the true relationship is not known, there is not enough information to exactly tell whether test dataset RSS will be better with a polynomial fit; 























